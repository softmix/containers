diff --git a/utils.py b/utils.py
index 45f82cd..78577a2 100644
--- a/utils.py
+++ b/utils.py
@@ -15,26 +15,29 @@ def load_models() -> Dict[str, Tuple[PreTrainedModel, PreTrainedTokenizer, Model
     models = {}
     for family in config.MODEL_FAMILIES.values():
         for model_config in family:
-            backend_config = model_config.backend
-
-            logger.info(f"Loading tokenizer for {backend_config.repository}")
-            tokenizer = AutoTokenizer.from_pretrained(backend_config.repository, add_bos_token=False, use_fast=False)
-
-            logger.info(
-                f"Loading model {backend_config.repository} with adapter {backend_config.adapter} in {config.TORCH_DTYPE}"
-            )
-            # We set use_fast=False since LlamaTokenizerFast takes a long time to init
-            model = AutoDistributedModelForCausalLM.from_pretrained(
-                backend_config.repository,
-                active_adapter=backend_config.adapter,
-                torch_dtype=config.TORCH_DTYPE,
-                initial_peers=config.INITIAL_PEERS,
-                max_retries=3,
-            )
-            model = model.to(config.DEVICE)
-
-            for key in [backend_config.key] + list(backend_config.aliases):
-                models[key] = model, tokenizer, backend_config
+            try:
+                backend_config = model_config.backend
+
+                logger.info(f"Loading tokenizer for {backend_config.repository}")
+                tokenizer = AutoTokenizer.from_pretrained(backend_config.repository, add_bos_token=False, use_fast=False)
+
+                logger.info(
+                    f"Loading model {backend_config.repository} with adapter {backend_config.adapter} in {config.TORCH_DTYPE}"
+                )
+                # We set use_fast=False since LlamaTokenizerFast takes a long time to init
+                model = AutoDistributedModelForCausalLM.from_pretrained(
+                    backend_config.repository,
+                    active_adapter=backend_config.adapter,
+                    torch_dtype=config.TORCH_DTYPE,
+                    initial_peers=config.INITIAL_PEERS,
+                    max_retries=3,
+                )
+                model = model.to(config.DEVICE)
+
+                for key in [backend_config.key] + list(backend_config.aliases):
+                    models[key] = model, tokenizer, backend_config
+            except Exception as e:
+                print(f"Failed to load model {model_config.backend.repository} with error: {e}")
     return models
 
 
